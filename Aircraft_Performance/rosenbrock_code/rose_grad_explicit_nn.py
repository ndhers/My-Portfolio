# -*- coding: utf-8 -*-
"""rose_grad_nn_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vzNVE_rh5h-ObZEzQENniV7syGW6lt0m
"""

import random

N = 1000
X = []
max_z = 0

for j in range(N):
  X_local = []
  for i in range(2):
    X_rand = random.uniform(-3, 3) # uniformly distributed between (-3, +3)
    X_local.append(X_rand)
  X_local.append((1-X_local[0])**2 + 100* ((X_local[1]-X_local[0]**2))**2)
  if abs(X_local[2]) > max_z:
    max_z = abs(X_local[2])
  X.append(X_local)

#print(max_z)

for j in range(N):
  X[j][2] = X[j][2]/max_z

training_dataset = X



#print(X)
import math
from math import exp
from random import seed
from random import random
import matplotlib
from matplotlib import pyplot
 
# Let's start by setting up the network (i.e. number of layers and neurons per layer)
def network_setup(number_input_nodes, number_hlayer1_nodes, number_hlayer2_nodes, number_output_nodes):
  network = []
  hidden_layer1 = [{'weights':[random() for i in range(number_input_nodes + 1)]} for i in range(number_hlayer1_nodes)]
  network.append(hidden_layer1)
  hidden_layer2 = [{'weights':[random() for i in range(number_hlayer1_nodes + 1)]} for i in range(number_hlayer2_nodes)]
  network.append(hidden_layer2)
  output_layer = [{'weights':[random() for i in range(number_hlayer2_nodes + 1)]} for i in range(number_output_nodes)]
  network.append(output_layer)
  return network
 
# Let's define a function to evaluate the response signal that is fed to the network (feedfoward)
def forward_propagation(network, sample):
	inputs = sample
	i = 0
	for layer in network:
		following_inputs = []
		for neuron in layer:
			h = activate(neuron['weights'], inputs)
			if i == 2:
				neuron['output'] = activation_function_tanh(h)
			else:
				neuron['output'] = activation_function_sigmoid(h)
			following_inputs.append(neuron['output'])
		inputs = following_inputs
		i = i+1
	return inputs

# Let's define a function that computes the input to each neuron
def activate(weights, inputs):
	bias = weights[-1]
	h = bias
	for i in range(len(weights)-1):
		h += weights[i] * inputs[i]
	return h
 
# Let's define the activation function, here we use the sigmoid
def activation_function_sigmoid(h):
	G = 1.0 / (1.0 + exp(-h))
	return G
 
 # Let's define the activation function, here we use the tanh for output between [-1,1]
def activation_function_tanh(h):
	G = math.sinh(h)/math.cosh(h)
	return G

 
# For gradient descent, we need derivative of our sigmoid function g in hidden layers
def activation_function_deriv_sigmoid(output):
	dG = output * (1.0 - output)
	return dG

# For gradient descent, we need derivative of our tanh function g in last layer
def activation_function_deriv_tanh(output):
	dG = (1.0 - output**2)
	return dG
 
# Let's define a function for the backpropagation algorithm and to store the error in each neuron
def backpropagation(network, target):
	net_length = len(network)
	for i in reversed(range(net_length)):
		error_list = []
		layer = network[i]
		if i == len(network)-1: # i.e. last layer
			for j in range(len(layer)):
				neuron = layer[j]
				error_list.append(target - neuron['output'])
		else:
			for j in range(len(layer)):
				error = 0.0
				for neuron in network[i + 1]:
					error += (neuron['weights'][j] * neuron['delta'])
				error_list.append(error)

		for j in range(len(layer)):
			neuron = layer[j]
			if i == len(network)-1:
				neuron['delta'] = error_list[j] * activation_function_deriv_tanh(neuron['output'])
			else:
				neuron['delta'] = error_list[j] * activation_function_deriv_sigmoid(neuron['output'])
 
# Using learning rate, delta value computed in backpropagation algorithm, we can update our weights
def weight_update(network, sample, learning_rate):
	for i in range(len(network)):
		inputs = sample[:-1]
		if i != 0:
			inputs = [neuron['output'] for neuron in network[i - 1]]
		for neuron in network[i]:
			for j in range(len(inputs)):
				neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]
			neuron['weights'][-1] += learning_rate * neuron['delta']

# Let's train the network for a given number of epochs (i.e. passes over the entire dataset)
def network_training(network, training_set, learning_rate, epoch_total):
  error_list = []
  for epoch in range(epoch_total):
    error_sum = 0
    i = 0
    for sample in training_set:
    	outputs = forward_propagation(network, sample)
    	target = sample[-1]
    	error_sum += (target-outputs[0])**2
    	backpropagation(network, target)
    	#if i%16 == 0: # batch_size of 16
    	weight_update(network, sample, learning_rate)
    	i = i + 1
    error_sum = error_sum/i
    error_list.append(error_sum)
    print('>epoch=%d, lrate=%.3f, error=%.6f' % (epoch, learning_rate, error_sum))
  return error_list


# Let's call the appropriate functions to set up and train the network

number_input_nodes = len(training_dataset[0]) - 1
number_output_nodes = 1
number_hlayer1_nodes = 10
number_hlayer2_nodes = 10
learning_rate = 0.42
epoch_total = 100
network = network_setup(number_input_nodes, number_hlayer1_nodes, number_hlayer2_nodes, number_output_nodes)
error_list = network_training(network, training_dataset, learning_rate, epoch_total)

#for layer in network:
#	print(layer)

# Let's look at a graph plotting the error vs epochs

epoch_list = []
number_of_epochs = 100
for i in range(number_of_epochs):
  epoch_list.append(i)

matplotlib.pyplot.plot(epoch_list,error_list)
matplotlib.pyplot.title('Error vs epochs')
matplotlib.pyplot.xlabel('Epochs')
matplotlib.pyplot.ylabel('Error')


# Let's make predictions based on our trained network
# Here we look at the point [1,1] whose target value should be 0 for the Rosenbrock Problem

def predict(network, sample):
	outputs = forward_propagation(network, sample)
	return outputs[0]

X = [1,1]

prediction = predict(network,X)

print(prediction)
print(prediction*max_z)

matplotlib.pyplot.show()