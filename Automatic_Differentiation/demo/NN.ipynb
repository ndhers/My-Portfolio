{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162647a8",
   "metadata": {},
   "source": [
    "# Neural networks with `autodiff`\n",
    "This brief demo showcases an important use case of automatic differentiation:\n",
    "neural networks. We implement two simple neural network models using\n",
    "our `autodiff` package for a handwritten digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95a8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autodiff.vector as vc\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45275cd",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "We use a boiled-down version of MNIST with only two digits: 0 and 1. The images\n",
    "are also scaled down to 8 by 8 pixels, rather than the full 28 by 28 pixel\n",
    "images of MNIST proper. These simplifications enable the demo to run\n",
    "quickly on a variety of hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a2caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(n_class=2, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766af9b",
   "metadata": {},
   "source": [
    "## Neural Network I\n",
    "Below, we implement a simple feed-forward neural network with two fully-\n",
    "connected layers:\n",
    "\n",
    "* **First layer:** 5 Neurons, Linear Activation\n",
    "* **Second layer:** output layer, 1 Neuron, Sigmoid activation\n",
    "\n",
    "The model is trained with binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ae5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter: 0]  label: 1   prob: 0.7555   loss: 0.2804\n",
      "[iter: 1]  label: 1   prob: 0.5642   loss: 0.5723\n",
      "[iter: 2]  label: 0   prob: 0.2459   loss: 0.2822\n",
      "[iter: 3]  label: 1   prob: 0.9441   loss: 0.0575\n",
      "[iter: 4]  label: 1   prob: 0.8671   loss: 0.1426\n",
      "[iter: 5]  label: 1   prob: 0.9789   loss: 0.0213\n",
      "[iter: 6]  label: 0   prob: 0.0408   loss: 0.0417\n",
      "[iter: 7]  label: 1   prob: 0.9998   loss: 0.0002\n",
      "[iter: 8]  label: 0   prob: 0.0003   loss: 0.0003\n",
      "[iter: 9]  label: 0   prob: 0.0155   loss: 0.0156\n",
      "[iter: 10]  label: 1   prob: 0.9970   loss: 0.0030\n",
      "[iter: 11]  label: 0   prob: 0.0826   loss: 0.0863\n",
      "[iter: 12]  label: 1   prob: 0.9984   loss: 0.0016\n",
      "[iter: 13]  label: 0   prob: 0.0014   loss: 0.0014\n",
      "[iter: 14]  label: 0   prob: 0.0084   loss: 0.0084\n",
      "[iter: 15]  label: 1   prob: 0.9929   loss: 0.0071\n",
      "[iter: 16]  label: 1   prob: 0.9998   loss: 0.0002\n",
      "[iter: 17]  label: 0   prob: 0.0005   loss: 0.0005\n",
      "[iter: 18]  label: 1   prob: 0.9498   loss: 0.0515\n",
      "[iter: 19]  label: 0   prob: 0.0019   loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "# Simple NN example, 2 layers, linear activation for the first layer and sigmoid activation for the second\n",
    "d = 8 * 8\n",
    "l1 = 5\n",
    "l2 = 1\n",
    "param_val = np.array([np.random.randn()/d for _ in range(l1*d)]\n",
    "                     + [np.random.randn()/l1 for _ in range(l2*l1)])\n",
    "\n",
    "\n",
    "g = vc.vec_gen()\n",
    "\n",
    "t11 = g.generate(d)\n",
    "t12 = g.generate(d)\n",
    "t13 = g.generate(d)\n",
    "t14 = g.generate(d)\n",
    "t15 = g.generate(d)\n",
    "\n",
    "t2 = g.generate(l1)\n",
    "\n",
    "def nn(input_indexs, param_val, step_size = 0.1, cnt=0):\n",
    "    L_ders = np.zeros((len(input_indexs), l1*d + l2*l1))\n",
    "    result = param_val\n",
    "    for i, input_index in enumerate(input_indexs):\n",
    "        # normalise the input\n",
    "        input_ = X_train[input_index, :]/16\n",
    "\n",
    "        z11 = vc.dot(t11, input_)\n",
    "        z12 = vc.dot(t12, input_)\n",
    "        z13 = vc.dot(t13, input_)\n",
    "        z14 = vc.dot(t14, input_)\n",
    "        z15 = vc.dot(t15, input_)\n",
    "        z1 = vc.concat([z11,z12,z13,z14,z15])\n",
    "        a1 = z1\n",
    "        \n",
    "        z2 = vc.concat([vc.dot(t2, a1)])\n",
    "        f = vc.sigmoid(z2)\n",
    "        y = y_train[input_index]\n",
    "\n",
    "        L = -y * vc.log(f) - (1-y) * vc.log(1-f)\n",
    "        L_der = L.quickderiv(param_val)\n",
    "        L_ders[i,:] = L_der.flatten()\n",
    "    result = param_val - np.sum(L_ders, axis=0).flatten() * step_size\n",
    "    f_val = f.quickeval(result)\n",
    "    L_val = L.quickeval(result)\n",
    "    print(f'[iter: {cnt}]  label: {y}   prob: {f_val[0]:.4f}   loss: {L_val[0]:.4f}')\n",
    "    return result\n",
    "\n",
    "for j in range(20):\n",
    "    input_indexs = np.random.choice(X_train.shape[0], 10, replace=False)\n",
    "    param_val = nn(input_indexs, param_val, step_size=0.1, cnt=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b73fa",
   "metadata": {},
   "source": [
    "## Neural Network II\n",
    "This more complex neural network features three fully connected layers:\n",
    "* **First layer:** 5 Neurons, ReLU Activation\n",
    "* **Second layer:** 2 Neurons, Linear Activation\n",
    "* **Third layer:** output layer, 1 Neuron, Sigmoid activation\n",
    "\n",
    "The model is trained with binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84c6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter: 0]  label: 0   prob: 0.5049   loss: 0.7029\n",
      "[iter: 1]  label: 1   prob: 0.5180   loss: 0.6578\n",
      "[iter: 2]  label: 1   prob: 0.5220   loss: 0.6501\n",
      "[iter: 3]  label: 0   prob: 0.4929   loss: 0.6791\n",
      "[iter: 4]  label: 0   prob: 0.4847   loss: 0.6630\n",
      "[iter: 5]  label: 0   prob: 0.4784   loss: 0.6508\n",
      "[iter: 6]  label: 0   prob: 0.4793   loss: 0.6526\n",
      "[iter: 7]  label: 0   prob: 0.3659   loss: 0.4555\n",
      "[iter: 8]  label: 0   prob: 0.0768   loss: 0.0800\n",
      "[iter: 9]  label: 0   prob: 0.5338   loss: 0.7631\n",
      "[iter: 10]  label: 1   prob: 0.9381   loss: 0.0639\n",
      "[iter: 11]  label: 0   prob: 0.3799   loss: 0.4779\n",
      "[iter: 12]  label: 0   prob: 0.1847   loss: 0.2042\n",
      "[iter: 13]  label: 1   prob: 0.9938   loss: 0.0063\n",
      "[iter: 14]  label: 0   prob: 0.0175   loss: 0.0177\n",
      "[iter: 15]  label: 0   prob: 0.0092   loss: 0.0092\n",
      "[iter: 16]  label: 0   prob: 0.0042   loss: 0.0042\n",
      "[iter: 17]  label: 1   prob: 0.9961   loss: 0.0039\n",
      "[iter: 18]  label: 1   prob: 0.9996   loss: 0.0004\n",
      "[iter: 19]  label: 1   prob: 0.9984   loss: 0.0016\n"
     ]
    }
   ],
   "source": [
    "d = 8 * 8\n",
    "l1 = 5\n",
    "l2 = 2\n",
    "l3 = 1\n",
    "\n",
    "param_val = np.array([np.random.randn()/d for _ in range(l1*d)]\n",
    "                     + [np.random.randn()/l1 for _ in range(l2*l1)]\n",
    "                     + [np.random.randn()/l2 for _ in range(l3*l2)])\n",
    "\n",
    "\n",
    "g = vc.vec_gen()\n",
    "\n",
    "t11 = g.generate(d)\n",
    "t12 = g.generate(d)\n",
    "t13 = g.generate(d)\n",
    "t14 = g.generate(d)\n",
    "t15 = g.generate(d)\n",
    "\n",
    "t21 = g.generate(l1)\n",
    "t22 = g.generate(l1)\n",
    "\n",
    "t3 = g.generate(l2)\n",
    "\n",
    "def nn(input_indexs, param_val, step_size = 0.1, cnt=0):\n",
    "    L_ders = np.zeros((len(input_indexs), l1*d + l2*l1 + l3*l2))\n",
    "    result = param_val\n",
    "    for i, input_index in enumerate(input_indexs):\n",
    "        # normalise the input\n",
    "        \n",
    "        # layer 1\n",
    "        input_ = X_train[input_index, :]/16\n",
    "\n",
    "        z11 = vc.dot(t11, input_)\n",
    "        z12 = vc.dot(t12, input_)\n",
    "        z13 = vc.dot(t13, input_)\n",
    "        z14 = vc.dot(t14, input_)\n",
    "        z15 = vc.dot(t15, input_)\n",
    "        z1 = vc.concat([z11,z12,z13,z14,z15])\n",
    "        a1 = vc.ReLU(z1)\n",
    "        \n",
    "        # layer 2\n",
    "        \n",
    "        z21 = vc.dot(t21, a1)\n",
    "        z22 = vc.dot(t22, a1)\n",
    "        z2 = vc.concat([z21, z22])\n",
    "        a2 = z2\n",
    "        \n",
    "        # layer 3\n",
    "        \n",
    "        z3 = vc.concat([vc.dot(t3, a2)])\n",
    "        f = vc.sigmoid(z3)\n",
    "        y = y_train[input_index]\n",
    "\n",
    "        L = -y * vc.log(f) - (1-y) * vc.log(1-f)\n",
    "        L_der = L.quickderiv(param_val)\n",
    "        L_ders[i,:] = L_der.flatten()\n",
    "    result = param_val - np.sum(L_ders, axis=0).flatten() * step_size\n",
    "    f_val = f.quickeval(result)\n",
    "    L_val = L.quickeval(result)\n",
    "    print(f'[iter: {cnt}]  label: {y}   prob: {f_val[0]:.4f}   loss: {L_val[0]:.4f}')\n",
    "    return result\n",
    "\n",
    "for j in range(20):\n",
    "    input_indexs = np.random.choice(X_train.shape[0], 10, replace=False)\n",
    "    param_val = nn(input_indexs, param_val, step_size=0.1, cnt=j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
